{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1) EDA & Stratified Splits (70/15/15)"
   ],
   "metadata": {
    "id": "PM1Adz4pfB7a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1_eda_and_splits_from_zip.ipynb / .py\n",
    "# -------------------------------------------------\n",
    "# EDA + stratified splits directly from a ZIP on Google Drive\n",
    "# No unzip needed. Reads images inside the archive using zipfile.ZipFile.\n",
    "# -------------------------------------------------\n",
    "\n",
    "import os, sys, io, json, random, subprocess\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "\n",
    "# Core deps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try installing ImageHash if missing (robust, works in Colab & plain Python)\n",
    "def ensure_imagehash():\n",
    "    try:\n",
    "        import imagehash  # noqa: F401\n",
    "        return True\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ImageHash\"], stdout=subprocess.DEVNULL)\n",
    "            import imagehash  # noqa: F401\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "_HAS_IMAGEHASH = ensure_imagehash()\n",
    "\n",
    "# Try to mount Google Drive if we're in Colab\n",
    "def maybe_mount_drive():\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        from google.colab import drive  # type: ignore\n",
    "        # Only mount if not already mounted\n",
    "        if not Path(\"/content/drive\").exists() or not any(Path(\"/content/drive\").iterdir()):\n",
    "            drive.mount('/content/drive', force_remount=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "_IN_COLAB = maybe_mount_drive()\n",
    "\n",
    "# -------------------\n",
    "# CONFIG\n",
    "# -------------------\n",
    "# CHANGE THIS to your actual ZIP in Drive or local filesystem:\n",
    "DATA_ZIP = Path(\"/content/drive/MyDrive/Agriculture_Dataset/Tamarind Image Dataset Healthy and Unhealthy Categories.zip\")  # e.g., /content/drive/MyDrive/.../tamarind.zip\n",
    "\n",
    "OUT_DIR  = Path(\"work_tamarind\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "VALID_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "# -------------------\n",
    "# SAFETY CHECKS\n",
    "# -------------------\n",
    "if not DATA_ZIP.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"ZIP not found: {DATA_ZIP}\\n\"\n",
    "        \"Please set DATA_ZIP to your actual path (e.g., '/content/drive/MyDrive/.../tamarind.zip').\"\n",
    "    )\n",
    "\n",
    "# -------------------\n",
    "# HASH HELPERS\n",
    "# -------------------\n",
    "def phash_from_pil_im(im: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Return a perceptual hash hex string.\n",
    "    Uses imagehash.phash if available, otherwise falls back to a simple aHash (8x8).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if _HAS_IMAGEHASH:\n",
    "            import imagehash\n",
    "            return str(imagehash.phash(im.convert(\"RGB\")))\n",
    "    except Exception:\n",
    "        pass  # fall back to aHash below\n",
    "\n",
    "    # Fallback: average-hash (aHash) 8x8 -> 64 bits -> 16 hex chars\n",
    "    im_small = im.convert(\"L\").resize((8, 8))\n",
    "    arr = np.asarray(im_small, dtype=np.float32)\n",
    "    avg = arr.mean()\n",
    "    bits = (arr > avg).astype(np.uint8).flatten()\n",
    "    out = 0\n",
    "    for b in bits:\n",
    "        out = (out << 1) | int(b)\n",
    "    return f\"{out:016x}\"\n",
    "\n",
    "# -------------------\n",
    "# ZIP HELPERS\n",
    "# -------------------\n",
    "def is_image_name(name: str) -> bool:\n",
    "    return Path(name).suffix.lower() in VALID_EXTS\n",
    "\n",
    "def normalize_zip_path(name: str) -> str:\n",
    "    p = name.replace(\"\\\\\", \"/\")\n",
    "    if p.startswith(\"./\"):\n",
    "        p = p[2:]\n",
    "    return p\n",
    "\n",
    "def infer_label_domain_from_parts(parts):\n",
    "    \"\"\"\n",
    "    Expected: .../<Healthy|Unhealthy>/<Shelled|Unshelled|Mixed>/image.ext\n",
    "    Returns ('Healthy'|'Unhealthy'|'Unknown', 'Shelled'|'Unshelled'|'Mixed'|'Unknown')\n",
    "    \"\"\"\n",
    "    label = \"Unknown\"\n",
    "    domain = \"Unknown\"\n",
    "    for p in parts:\n",
    "        low = p.lower()\n",
    "        if low in [\"healthy\", \"unhealthy\"]:\n",
    "            label = \"Healthy\" if low == \"healthy\" else \"Unhealthy\"\n",
    "        if low in [\"shelled\", \"unshelled\", \"mixed\"]:\n",
    "            domain = {\"shelled\":\"Shelled\", \"unshelled\":\"Unshelled\", \"mixed\":\"Mixed\"}[low]\n",
    "    return label, domain\n",
    "\n",
    "def zip_image_members(zf: zipfile.ZipFile):\n",
    "    for name in zf.namelist():\n",
    "        if not name or name.endswith(\"/\"):\n",
    "            continue\n",
    "        nm = normalize_zip_path(name)\n",
    "        if is_image_name(nm):\n",
    "            yield nm\n",
    "\n",
    "def open_image_from_zip(zippath: Path, member: str) -> Image.Image:\n",
    "    with zipfile.ZipFile(zippath, \"r\") as zf:\n",
    "        with zf.open(member, \"r\") as f:\n",
    "            data = f.read()\n",
    "    im = Image.open(io.BytesIO(data))\n",
    "    im.load()\n",
    "    return im\n",
    "\n",
    "# -------------------\n",
    "# STEP 1: Build metadata table\n",
    "# -------------------\n",
    "rows = []\n",
    "with zipfile.ZipFile(DATA_ZIP, \"r\") as zf:\n",
    "    all_img_members = list(zip_image_members(zf))\n",
    "    if len(all_img_members) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"No images found in ZIP: {DATA_ZIP}\\n\"\n",
    "            f\"Accepted extensions: {sorted(VALID_EXTS)}\"\n",
    "        )\n",
    "    for member in all_img_members:\n",
    "        parts = [p for p in member.split(\"/\") if p]\n",
    "        label, domain = infer_label_domain_from_parts(parts)\n",
    "        rows.append({\n",
    "            \"zip_path\": str(DATA_ZIP.as_posix()),\n",
    "            \"zip_member\": member,\n",
    "            \"path\": f\"zip://{DATA_ZIP.as_posix()}::{member}\",  # virtual path for convenience\n",
    "            \"label\": label,\n",
    "            \"domain\": domain,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"zip_member\"]).reset_index(drop=True)\n",
    "df[\"label_id\"]  = df[\"label\"].map({\"Healthy\":0, \"Unhealthy\":1})\n",
    "df[\"domain_id\"] = df[\"domain\"].map({\"Shelled\":0, \"Unshelled\":1, \"Mixed\":2})\n",
    "\n",
    "df.to_csv(OUT_DIR/\"metadata_raw.csv\", index=False)\n",
    "print(\"Metadata:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# -------------------\n",
    "# STEP 2: Basic counts\n",
    "# -------------------\n",
    "from collections import Counter\n",
    "print(\"\\n=== Overall counts ===\")\n",
    "print(\"Total images:\", len(df))\n",
    "print(\"Classes:\", Counter(df[\"label\"]))\n",
    "print(\"Domains:\", Counter(df[\"domain\"]))\n",
    "\n",
    "ct = pd.crosstab(df[\"label\"], df[\"domain\"])\n",
    "print(\"\\nClass x Domain\\n\", ct)\n",
    "\n",
    "# -------------------\n",
    "# STEP 3: Image resolution variety + invalids\n",
    "# -------------------\n",
    "def get_size_from_zip(member: str):\n",
    "    try:\n",
    "        with zipfile.ZipFile(DATA_ZIP, \"r\") as zf:\n",
    "            with zf.open(member, \"r\") as f:\n",
    "                with Image.open(io.BytesIO(f.read())) as im:\n",
    "                    im.load()\n",
    "                    return im.size  # (W, H)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"\\nReading image sizes (may take a while on large ZIPs)...\")\n",
    "df[\"size\"] = df[\"zip_member\"].apply(get_size_from_zip)\n",
    "invalid = df[\"size\"].isna().sum()\n",
    "print(f\"Invalid images (cannot open): {invalid}\")\n",
    "\n",
    "df_valid = df.dropna(subset=[\"size\"]).copy()\n",
    "if len(df_valid) == 0:\n",
    "    raise RuntimeError(\"All images failed to openâ€”please check the ZIP integrity or formats.\")\n",
    "\n",
    "df_valid[\"W\"] = df_valid[\"size\"].apply(lambda s: s[0])\n",
    "df_valid[\"H\"] = df_valid[\"size\"].apply(lambda s: s[1])\n",
    "\n",
    "res_counts = (\n",
    "    df_valid.groupby([\"W\",\"H\"])\n",
    "    .size().reset_index(name=\"count\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "res_counts.to_csv(OUT_DIR/\"resolutions.csv\", index=False)\n",
    "print(\"\\nTop resolutions:\\n\", res_counts.head(10))\n",
    "\n",
    "# Plots (safe to skip if running headless)\n",
    "try:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df_valid[\"W\"], bins=30)\n",
    "    plt.title(\"Width distribution\")\n",
    "    plt.xlabel(\"Width (px)\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df_valid[\"H\"], bins=30)\n",
    "    plt.title(\"Height distribution\")\n",
    "    plt.xlabel(\"Height (px)\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception as _e:\n",
    "    print(\"Plotting skipped (no display backend).\")\n",
    "\n",
    "# -------------------\n",
    "# STEP 4: Duplicate groups via perceptual hashing\n",
    "# -------------------\n",
    "print(\"\\nComputing perceptual hashes for duplicate detection (may take time)...\")\n",
    "\n",
    "def phash_from_zip(member: str) -> str | None:\n",
    "    try:\n",
    "        with zipfile.ZipFile(DATA_ZIP, \"r\") as zf:\n",
    "            with zf.open(member, \"r\") as f:\n",
    "                with Image.open(io.BytesIO(f.read())) as im:\n",
    "                    return phash_from_pil_im(im)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_valid[\"phash\"] = df_valid[\"zip_member\"].apply(phash_from_zip)\n",
    "\n",
    "dup_groups = (\n",
    "    df_valid.dropna(subset=[\"phash\"])\n",
    "            .groupby(\"phash\")[\"zip_member\"]\n",
    "            .apply(list).reset_index()\n",
    ")\n",
    "dup_groups = dup_groups[dup_groups[\"zip_member\"].apply(lambda L: len(L) > 1)]\n",
    "\n",
    "dup_groups_out = [\n",
    "    {\"phash\": row[\"phash\"], \"members\": row[\"zip_member\"]}\n",
    "    for _, row in dup_groups.iterrows()\n",
    "]\n",
    "with open(OUT_DIR/\"duplicate_groups.json\", \"w\") as f:\n",
    "    json.dump(dup_groups_out, f, indent=2)\n",
    "\n",
    "print(f\"Duplicate groups (size>1): {len(dup_groups)}\")\n",
    "\n",
    "# -------------------\n",
    "# STEP 5: Class imbalance quick view\n",
    "# -------------------\n",
    "class_counts = df_valid[\"label\"].value_counts(dropna=False)\n",
    "class_pct = (class_counts / class_counts.sum() * 100).round(2)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(pd.DataFrame({\"count\": class_counts, \"percent\": class_pct}))\n",
    "\n",
    "# -------------------\n",
    "# STEP 6: Stratified 70/15/15 splits (by label+domain)\n",
    "# -------------------\n",
    "df_valid[\"strat_key\"] = df_valid[\"label\"].astype(str) + \"_\" + df_valid[\"domain\"].astype(str)\n",
    "\n",
    "# Validate stratification keys have at least 2 classes; otherwise, just do a simple split\n",
    "unique_keys = df_valid[\"strat_key\"].nunique()\n",
    "use_stratified = unique_keys > 1 and df_valid[\"strat_key\"].value_counts().min() >= 3  # enough samples per key\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if use_stratified:\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=RANDOM_SEED)\n",
    "    train_idx, temp_idx = next(sss1.split(df_valid, df_valid[\"strat_key\"]))\n",
    "    train_df = df_valid.iloc[train_idx].copy()\n",
    "    temp_df  = df_valid.iloc[temp_idx].copy()\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=RANDOM_SEED)\n",
    "    val_idx, test_idx = next(sss2.split(temp_df, temp_df[\"strat_key\"]))\n",
    "    val_df  = temp_df.iloc[val_idx].copy()\n",
    "    test_df = temp_df.iloc[test_idx].copy()\n",
    "else:\n",
    "    # Fallback: non-stratified deterministic splits (small/degenerate cases)\n",
    "    n = len(df_valid)\n",
    "    idx = np.random.RandomState(RANDOM_SEED).permutation(n)\n",
    "    n_train = int(0.70 * n)\n",
    "    n_val   = int(0.15 * n)\n",
    "    train_df = df_valid.iloc[idx[:n_train]].copy()\n",
    "    val_df   = df_valid.iloc[idx[n_train:n_train+n_val]].copy()\n",
    "    test_df  = df_valid.iloc[idx[n_train+n_val:]].copy()\n",
    "\n",
    "def save_split(name, d):\n",
    "    cols_order = [\"zip_path\", \"zip_member\", \"path\", \"label\", \"label_id\", \"domain\", \"domain_id\"]\n",
    "    cols_order = [c for c in cols_order if c in d.columns]\n",
    "    d_out = d[cols_order].copy()\n",
    "    d_out.to_csv(OUT_DIR/f\"{name}.csv\", index=False)\n",
    "    print(f\"{name}: {len(d_out)}\")\n",
    "\n",
    "save_split(\"train\", train_df)\n",
    "save_split(\"val\",   val_df)\n",
    "save_split(\"test\",  test_df)\n",
    "\n",
    "def split_summary(name, d):\n",
    "    print(f\"\\n=== {name} summary ===\")\n",
    "    try:\n",
    "        print(\"Label distribution:\\n\", d[\"label\"].value_counts(normalize=True).round(4))\n",
    "        print(\"Domain distribution:\\n\", d[\"domain\"].value_counts(normalize=True).round(4))\n",
    "        print(\"Label x Domain:\\n\", pd.crosstab(d[\"label\"], d[\"domain\"], normalize='all').round(4))\n",
    "    except Exception as _e:\n",
    "        print(\"Summary skipped due to insufficient categories.\")\n",
    "\n",
    "split_summary(\"train\", train_df)\n",
    "split_summary(\"val\",   val_df)\n",
    "split_summary(\"test\",  test_df)\n",
    "\n",
    "print(\"\\nWrote:\")\n",
    "print(\" - metadata_raw.csv\")\n",
    "print(\" - resolutions.csv\")\n",
    "print(\" - duplicate_groups.json\")\n",
    "print(\" - train.csv, val.csv, test.csv\")\n",
    "print(f\"\\n(All in: {OUT_DIR.resolve()})\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u53FLNmqhGn7",
    "outputId": "dbc742bb-7fb5-48d2-ad4e-9ed2e8981b0f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "Metadata: (8432, 7)\n",
      "                                            zip_path  \\\n",
      "0  /content/drive/MyDrive/Agriculture_Dataset/Tam...   \n",
      "1  /content/drive/MyDrive/Agriculture_Dataset/Tam...   \n",
      "2  /content/drive/MyDrive/Agriculture_Dataset/Tam...   \n",
      "3  /content/drive/MyDrive/Agriculture_Dataset/Tam...   \n",
      "4  /content/drive/MyDrive/Agriculture_Dataset/Tam...   \n",
      "\n",
      "                                          zip_member  \\\n",
      "0  Tamarind Image Dataset Healthy and Unhealthy C...   \n",
      "1  Tamarind Image Dataset Healthy and Unhealthy C...   \n",
      "2  Tamarind Image Dataset Healthy and Unhealthy C...   \n",
      "3  Tamarind Image Dataset Healthy and Unhealthy C...   \n",
      "4  Tamarind Image Dataset Healthy and Unhealthy C...   \n",
      "\n",
      "                                                path    label   domain  \\\n",
      "0  zip:///content/drive/MyDrive/Agriculture_Datas...  Unknown  Unknown   \n",
      "1  zip:///content/drive/MyDrive/Agriculture_Datas...  Unknown  Unknown   \n",
      "2  zip:///content/drive/MyDrive/Agriculture_Datas...  Unknown  Unknown   \n",
      "3  zip:///content/drive/MyDrive/Agriculture_Datas...  Unknown  Unknown   \n",
      "4  zip:///content/drive/MyDrive/Agriculture_Datas...  Unknown  Unknown   \n",
      "\n",
      "   label_id  domain_id  \n",
      "0       NaN        NaN  \n",
      "1       NaN        NaN  \n",
      "2       NaN        NaN  \n",
      "3       NaN        NaN  \n",
      "4       NaN        NaN  \n",
      "\n",
      "=== Overall counts ===\n",
      "Total images: 8432\n",
      "Classes: Counter({'Unknown': 8432})\n",
      "Domains: Counter({'Unknown': 8432})\n",
      "\n",
      "Class x Domain\n",
      " domain   Unknown\n",
      "label           \n",
      "Unknown     8432\n",
      "\n",
      "Reading image sizes (may take a while on large ZIPs)...\n",
      "Invalid images (cannot open): 0\n",
      "\n",
      "Top resolutions:\n",
      "       W     H  count\n",
      "1  1440  1080   5423\n",
      "0   810  1080   3009\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Computing perceptual hashes for duplicate detection (may take time)...\n",
      "Duplicate groups (size>1): 323\n",
      "\n",
      "Class distribution:\n",
      "         count  percent\n",
      "label                  \n",
      "Unknown   8432    100.0\n",
      "train: 5902\n",
      "val: 1264\n",
      "test: 1266\n",
      "\n",
      "=== train summary ===\n",
      "Label distribution:\n",
      " label\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Domain distribution:\n",
      " domain\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Label x Domain:\n",
      " domain   Unknown\n",
      "label           \n",
      "Unknown      1.0\n",
      "\n",
      "=== val summary ===\n",
      "Label distribution:\n",
      " label\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Domain distribution:\n",
      " domain\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Label x Domain:\n",
      " domain   Unknown\n",
      "label           \n",
      "Unknown      1.0\n",
      "\n",
      "=== test summary ===\n",
      "Label distribution:\n",
      " label\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Domain distribution:\n",
      " domain\n",
      "Unknown    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Label x Domain:\n",
      " domain   Unknown\n",
      "label           \n",
      "Unknown      1.0\n",
      "\n",
      "Wrote:\n",
      " - metadata_raw.csv\n",
      " - resolutions.csv\n",
      " - duplicate_groups.json\n",
      " - train.csv, val.csv, test.csv\n",
      "\n",
      "(All in: /content/work_tamarind)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teacher Training: Swin-T + FiLM Domain Adapter + Simple MIL\n",
    "\n",
    "This version treats each image as a bag of fixed patches (grid). We extract patch features with Swin-T, then apply FiLM (gamma/beta from domain embedding), then attention-based MIL pooling to get a bag-level prediction. Includes optional domain contrastive loss."
   ],
   "metadata": {
    "id": "YluG_mTffFVm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 2_teacher_swin_film_mil_train.ipynb / .py\n",
    "\n",
    "import os, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_SPLIT_DIR = Path(\"work_tamarind\")\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LR = 2e-4\n",
    "NUM_CLASSES = 2\n",
    "DOMAINS = [\"Shelled\",\"Unshelled\",\"Mixed\"]\n",
    "DOMAIN2ID = {d:i for i,d in enumerate(DOMAINS)}\n",
    "IMG_SIZE = 384\n",
    "GRID = (3, 3)  # 3x3 patches per image for MIL\n",
    "TEMPERATURE_CL = 0.07  # for contrastive domain loss (optional)\n",
    "USE_DOMAIN_CONTRASTIVE = True\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class TamarindPatches(Dataset):\n",
    "    def __init__(self, csv_path, augment=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.augment = augment\n",
    "        self.tfm_train = A.Compose([\n",
    "            A.LongestMaxSize(IMG_SIZE),\n",
    "            A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "            A.Flip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.tfm_eval  = A.Compose([\n",
    "            A.LongestMaxSize(IMG_SIZE),\n",
    "            A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _grid_crops(self, img_tensor):\n",
    "        # img_tensor: C,H,W\n",
    "        C, H, W = img_tensor.shape\n",
    "        gh, gw = GRID\n",
    "        ph, pw = H // gh, W // gw\n",
    "        patches = []\n",
    "        for i in range(gh):\n",
    "            for j in range(gw):\n",
    "                patches.append(img_tensor[:, i*ph:(i+1)*ph, j*pw:(j+1)*pw])\n",
    "        return torch.stack(patches, dim=0)  # [P, C, ph, pw]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        p = r[\"path\"]\n",
    "        y = 0 if r[\"label\"] == \"Healthy\" else 1\n",
    "        d = DOMAIN2ID.get(r[\"domain\"], 0)\n",
    "\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            arr = np.array(im)\n",
    "\n",
    "        tfm = self.tfm_train if self.augment else self.tfm_eval\n",
    "        out = tfm(image=arr)[\"image\"]  # tensor C,H,W\n",
    "        patches = self._grid_crops(out)\n",
    "        return patches, torch.tensor(y).long(), torch.tensor(d).long(), p\n",
    "\n",
    "# -------------------------\n",
    "# FiLM Adapter + MIL Pool\n",
    "# -------------------------\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, feat_dim, num_domains=3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_domains, 128)\n",
    "        self.to_gamma = nn.Linear(128, feat_dim)\n",
    "        self.to_beta  = nn.Linear(128, feat_dim)\n",
    "\n",
    "    def forward(self, x, d):\n",
    "        # x: [B, P, D]\n",
    "        e = self.emb(d)                 # [B, 128]\n",
    "        gamma = self.to_gamma(e).unsqueeze(1)  # [B,1,D]\n",
    "        beta  = self.to_beta(e).unsqueeze(1)   # [B,1,D]\n",
    "        return x * (1 + gamma) + beta\n",
    "\n",
    "class AttnMILPool(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, feats):\n",
    "        # feats: [B, P, D]\n",
    "        a = self.attn(feats)                        # [B,P,1]\n",
    "        w = torch.softmax(a, dim=1)                 # [B,P,1]\n",
    "        bag = (w * feats).sum(dim=1)                # [B,D]\n",
    "        return bag, w\n",
    "\n",
    "class TeacherNet(nn.Module):\n",
    "    def __init__(self, backbone_name=\"swin_tiny_patch4_window7_224\", num_domains=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.feat_dim = self.backbone.num_features\n",
    "        self.film = FiLM(self.feat_dim, num_domains=num_domains)\n",
    "        self.mil  = AttnMILPool(self.feat_dim)\n",
    "        self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_patch(self, x):  # x: [B*P, C, H, W]\n",
    "        return self.backbone(x) # [B*P, D]\n",
    "\n",
    "    def forward(self, patches, domain_ids):\n",
    "        # patches: [B, P, C, h, w]\n",
    "        B, P, C, h, w = patches.shape\n",
    "        x = patches.view(B*P, C, h, w)\n",
    "        patch_feats = self.backbone(x)          # [B*P, D]\n",
    "        patch_feats = patch_feats.view(B, P, -1)  # [B,P,D]\n",
    "        film_feats  = self.film(patch_feats, domain_ids)  # [B,P,D]\n",
    "        bag, attn_w = self.mil(film_feats)     # [B,D], [B,P,1]\n",
    "        logits = self.head(bag)                # [B,2]\n",
    "        return logits, bag, film_feats, attn_w\n",
    "\n",
    "# -------------------------\n",
    "# Losses\n",
    "# -------------------------\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "def domain_contrastive_loss(bag_feats, domain_ids, temp=TEMPERATURE_CL):\n",
    "    \"\"\"\n",
    "    Simple supervised contrastive: pull same-domain bags together, push different apart.\n",
    "    bag_feats: [B, D] (L2-normalized)\n",
    "    domain_ids: [B]\n",
    "    \"\"\"\n",
    "    z = nn.functional.normalize(bag_feats, dim=1)\n",
    "    sim = torch.matmul(z, z.T) / temp  # [B,B]\n",
    "    mask = (domain_ids.unsqueeze(1) == domain_ids.unsqueeze(0)).float()\n",
    "    logits = sim - torch.eye(sim.size(0), device=sim.device) * 1e9  # remove self\n",
    "    # log_softmax over rows\n",
    "    logp = logits.log_softmax(dim=1)\n",
    "    # average log prob of positives\n",
    "    pos = logp * mask\n",
    "    denom = mask.sum(dim=1).clamp_min(1.0)\n",
    "    loss = -(pos.sum(dim=1) / denom).mean()\n",
    "    return loss\n",
    "\n",
    "# -------------------------\n",
    "# Train / Val Loop\n",
    "# -------------------------\n",
    "def run_train(train_csv, val_csv, save_path=\"teacher_swin_film_mil.pt\"):\n",
    "    train_ds = TamarindPatches(train_csv, augment=True)\n",
    "    val_ds   = TamarindPatches(val_csv,   augment=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = TeacherNet(num_domains=len(DOMAINS), num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for patches, y, d, _ in train_dl:\n",
    "            patches = patches.to(DEVICE)  # [B,P,C,h,w]\n",
    "            y = y.to(DEVICE)\n",
    "            d = d.to(DEVICE)\n",
    "\n",
    "            logits, bag, film_feats, _ = model(patches, d)\n",
    "            loss = ce(logits, y)\n",
    "            if USE_DOMAIN_CONTRASTIVE:\n",
    "                loss += 0.05 * domain_contrastive_loss(bag, d)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * patches.size(0)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for patches, y, d, _ in val_dl:\n",
    "                patches = patches.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                d = d.to(DEVICE)\n",
    "                logits, _, _, _ = model(patches, d)\n",
    "                pred = logits.argmax(1)\n",
    "                preds.extend(pred.cpu().tolist())\n",
    "                gts.extend(y.cpu().tolist())\n",
    "\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        mf1 = f1_score(gts, preds, average=\"macro\")\n",
    "        print(f\"[Epoch {epoch}] train_loss={tr_loss/len(train_ds):.4f}  val_acc={acc:.4f}  val_macroF1={mf1:.4f}\")\n",
    "\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"  -> saved {save_path} (best macro-F1 {best_f1:.4f})\")\n",
    "\n",
    "    return save_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best = run_train(DATA_SPLIT_DIR/\"train.csv\", DATA_SPLIT_DIR/\"val.csv\")\n",
    "    print(\"Best model saved at:\", best)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "8uoOXv-SeKV8",
    "outputId": "0d733925-9109-44de-a9c9-0a96a9daa384"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-1211754278.py:39: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'albumentations' has no attribute 'Flip'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-1211754278.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 213\u001B[0;31m     \u001B[0mbest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_SPLIT_DIR\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;34m\"train.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDATA_SPLIT_DIR\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;34m\"val.csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    214\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Best model saved at:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-1211754278.py\u001B[0m in \u001B[0;36mrun_train\u001B[0;34m(train_csv, val_csv, save_path)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[0;31m# -------------------------\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrun_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_csv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_csv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msave_path\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"teacher_swin_film_mil.pt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m     \u001B[0mtrain_ds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTamarindPatches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_csv\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maugment\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m     \u001B[0mval_ds\u001B[0m   \u001B[0;34m=\u001B[0m \u001B[0mTamarindPatches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mval_csv\u001B[0m\u001B[0;34m,\u001B[0m   \u001B[0maugment\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m     \u001B[0mtrain_dl\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_workers\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpin_memory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdrop_last\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipython-input-1211754278.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, csv_path, augment)\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[0mA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLongestMaxSize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIMG_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m             \u001B[0mA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPadIfNeeded\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mIMG_SIZE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIMG_SIZE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mborder_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m             \u001B[0mA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFlip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m             \u001B[0mA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mRandomBrightnessContrast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0mA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mShiftScaleRotate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshift_limit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscale_limit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrotate_limit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'albumentations' has no attribute 'Flip'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Student Training with Knowledge Distillation (MobileNetV3 + KD)\n",
    "\n",
    "We distill logits (soft targets, temperature Ï„) and features (MSE). Optionally a contrastive KD between teacher/student features."
   ],
   "metadata": {
    "id": "ELR2qDNzfKCK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 3_student_kd_mobilenetv3.ipynb / .py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_SPLIT_DIR = Path(\"work_tamarind\")\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25\n",
    "LR = 2e-4\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 384\n",
    "GRID = (3,3)\n",
    "TAU = 3.0\n",
    "ALPHA_LOGITS = 0.7   # KD weight for logits\n",
    "BETA_FEAT   = 0.3    # KD weight for feature MSE\n",
    "GAMMA_CTR   = 0.0    # set >0.0 to use contrastive KD on patch features\n",
    "\n",
    "DOMAINS = [\"Shelled\",\"Unshelled\",\"Mixed\"]\n",
    "DOMAIN2ID = {d:i for i,d in enumerate(DOMAINS)}\n",
    "\n",
    "# -------- Dataset (same as teacher) --------\n",
    "class TamarindPatches(Dataset):\n",
    "    def __init__(self, csv_path, augment=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.augment = augment\n",
    "        self.tfm_train = A.Compose([\n",
    "            A.LongestMaxSize(IMG_SIZE),\n",
    "            A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "            A.Flip(p=0.5),\n",
    "            A.ColorJitter(p=0.4),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.tfm_eval  = A.Compose([\n",
    "            A.LongestMaxSize(IMG_SIZE),\n",
    "            A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _grid_crops(self, img_tensor):\n",
    "        C, H, W = img_tensor.shape\n",
    "        gh, gw = GRID\n",
    "        ph, pw = H // gh, W // gw\n",
    "        patches = []\n",
    "        for i in range(gh):\n",
    "            for j in range(gw):\n",
    "                patches.append(img_tensor[:, i*ph:(i+1)*ph, j*pw:(j+1)*pw])\n",
    "        return torch.stack(patches, dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        p = r[\"path\"]\n",
    "        y = 0 if r[\"label\"] == \"Healthy\" else 1\n",
    "        d = DOMAIN2ID.get(r[\"domain\"], 0)\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            arr = np.array(im)\n",
    "        tfm = self.tfm_train if self.augment else self.tfm_eval\n",
    "        out = tfm(image=arr)[\"image\"]\n",
    "        patches = self._grid_crops(out)\n",
    "        return patches, torch.tensor(y).long(), torch.tensor(d).long(), p\n",
    "\n",
    "# -------- Models --------\n",
    "class SimpleMIL(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=192):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, feats):\n",
    "        a = self.attn(feats)               # [B,P,1]\n",
    "        w = torch.softmax(a, dim=1)\n",
    "        bag = (w * feats).sum(dim=1)       # [B,D]\n",
    "        return bag, w\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self, backbone=\"mobilenetv3_large_100\", num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        self.feat_dim = self.backbone.num_features\n",
    "        self.mil = SimpleMIL(self.feat_dim)\n",
    "        self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, patches):\n",
    "        B,P,C,h,w = patches.shape\n",
    "        x = patches.view(B*P, C, h, w)\n",
    "        feats = self.backbone(x)           # [B*P, D]\n",
    "        feats = feats.view(B,P,-1)\n",
    "        bag, attn = self.mil(feats)\n",
    "        logits = self.head(bag)\n",
    "        return logits, bag, feats, attn\n",
    "\n",
    "class TeacherWrapper(nn.Module):\n",
    "    \"\"\"Load the trained teacher for KD (frozen).\"\"\"\n",
    "    def __init__(self, ckpt_path, num_domains=3, num_classes=2):\n",
    "        super().__init__()\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        # Recreate TeacherNet structure from Part 2:\n",
    "        import timm, torch.nn as nn\n",
    "        class FiLM(nn.Module):\n",
    "            def __init__(self, feat_dim, num_domains=3):\n",
    "                super().__init__()\n",
    "                self.emb = nn.Embedding(num_domains, 128)\n",
    "                self.to_gamma = nn.Linear(128, feat_dim)\n",
    "                self.to_beta  = nn.Linear(128, feat_dim)\n",
    "            def forward(self, x, d):\n",
    "                e = self.emb(d)\n",
    "                gamma = self.to_gamma(e).unsqueeze(1)\n",
    "                beta  = self.to_beta(e).unsqueeze(1)\n",
    "                return x * (1 + gamma) + beta\n",
    "\n",
    "        class AttnMILPool(nn.Module):\n",
    "            def __init__(self, in_dim, hidden=256):\n",
    "                super().__init__()\n",
    "                self.attn = nn.Sequential(\n",
    "                    nn.Linear(in_dim, hidden), nn.Tanh(),\n",
    "                    nn.Linear(hidden, 1)\n",
    "                )\n",
    "            def forward(self, feats):\n",
    "                a = self.attn(feats)\n",
    "                w = torch.softmax(a, dim=1)\n",
    "                bag = (w * feats).sum(dim=1)\n",
    "                return bag, w\n",
    "\n",
    "        class TeacherNet(nn.Module):\n",
    "            def __init__(self, backbone_name=\"swin_tiny_patch4_window7_224\", num_domains=3, num_classes=2):\n",
    "                super().__init__()\n",
    "                self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0, global_pool=\"avg\")\n",
    "                self.feat_dim = self.backbone.num_features\n",
    "                self.film = FiLM(self.feat_dim, num_domains=num_domains)\n",
    "                self.mil  = AttnMILPool(self.feat_dim)\n",
    "                self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "            def forward(self, patches, domain_ids):\n",
    "                B,P,C,h,w = patches.shape\n",
    "                x = patches.view(B*P, C, h, w)\n",
    "                pf = self.backbone(x).view(B,P,-1)\n",
    "                ff = self.film(pf, domain_ids)\n",
    "                bag, _ = self.mil(ff)\n",
    "                logits = self.head(bag)\n",
    "                return logits, bag, ff\n",
    "\n",
    "        self.teacher = TeacherNet(num_domains=len(DOMAINS), num_classes=NUM_CLASSES)\n",
    "        self.teacher.load_state_dict(torch.load(ckpt_path, map_location=\"cpu\"))\n",
    "        self.teacher.eval()\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, patches, domain_ids):\n",
    "        logits_t, bag_t, feats_t = self.teacher(patches, domain_ids)\n",
    "        return logits_t, bag_t, feats_t\n",
    "\n",
    "# -------- KD Losses --------\n",
    "ce = nn.CrossEntropyLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def kd_logits_loss(student_logits, teacher_logits, T=TAU):\n",
    "    # KLDiv between softened distributions\n",
    "    log_p_s = nn.functional.log_softmax(student_logits / T, dim=1)\n",
    "    p_t     = nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    return nn.functional.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T**2)\n",
    "\n",
    "def contrastive_kd(student_feats, teacher_feats, temp=0.07):\n",
    "    \"\"\"\n",
    "    Contrastive KD: align per-patch features from student to teacher for each image.\n",
    "    student_feats, teacher_feats: [B,P,D]\n",
    "    \"\"\"\n",
    "    B,P,D = student_feats.shape\n",
    "    s = nn.functional.normalize(student_feats, dim=-1)\n",
    "    t = nn.functional.normalize(teacher_feats, dim=-1)\n",
    "    loss = 0.0\n",
    "    for b in range(B):\n",
    "        sim = s[b] @ t[b].T / temp  # [P,P]\n",
    "        pos = torch.diag(sim)       # P positives\n",
    "        denom = torch.logsumexp(sim, dim=1)\n",
    "        loss += -(pos - denom).mean()\n",
    "    return loss / B\n",
    "\n",
    "# -------- Train --------\n",
    "def run_train(train_csv, val_csv, teacher_ckpt, save_path=\"student_mnv3_kd.pt\"):\n",
    "    train_ds = TamarindPatches(train_csv, augment=True)\n",
    "    val_ds   = TamarindPatches(val_csv,   augment=False)\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    student = StudentNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    teacher = TeacherWrapper(teacher_ckpt).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(student.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        student.train()\n",
    "        tr_loss = 0.0\n",
    "        for patches, y, d, _ in train_dl:\n",
    "            patches = patches.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            d = d.to(DEVICE)\n",
    "\n",
    "            # forward\n",
    "            logits_s, bag_s, feats_s, _ = student(patches)\n",
    "            with torch.no_grad():\n",
    "                logits_t, bag_t, feats_t = teacher(patches, d)\n",
    "\n",
    "            # losses\n",
    "            loss_ce = ce(logits_s, y)\n",
    "            loss_kd = kd_logits_loss(logits_s, logits_t, T=TAU)\n",
    "            loss_fm = mse(bag_s, bag_t)\n",
    "\n",
    "            loss = (1-ALPHA_LOGITS)*loss_ce + ALPHA_LOGITS*loss_kd + BETA_FEAT*loss_fm\n",
    "            if GAMMA_CTR > 0:\n",
    "                loss += GAMMA_CTR * contrastive_kd(feats_s, feats_t)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * patches.size(0)\n",
    "\n",
    "        # validation\n",
    "        student.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for patches, y, d, _ in val_dl:\n",
    "                patches = patches.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                logits_s, _, _, _ = student(patches)\n",
    "                preds.extend(torch.argmax(logits_s, 1).cpu().tolist())\n",
    "                gts.extend(y.cpu().tolist())\n",
    "\n",
    "        acc = accuracy_score(gts, preds)\n",
    "        mf1 = f1_score(gts, preds, average=\"macro\")\n",
    "        print(f\"[Epoch {epoch}] train_loss={tr_loss/len(train_ds):.4f}  val_acc={acc:.4f}  val_macroF1={mf1:.4f}\")\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            torch.save(student.state_dict(), save_path)\n",
    "            print(f\"  -> saved {save_path} (best macro-F1 {best_f1:.4f})\")\n",
    "\n",
    "    return save_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ckpt = \"teacher_swin_film_mil.pt\"  # from Part 2\n",
    "    best = run_train(DATA_SPLIT_DIR/\"train.csv\", DATA_SPLIT_DIR/\"val.csv\", ckpt)\n",
    "    print(\"Best student saved at:\", best)\n"
   ],
   "metadata": {
    "id": "rIOtSG9FeQsb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Evaluation + Cross-Domain Robustness\n",
    "\n",
    "Train on all domains (as you did), then slice by domain on the test set and also run train-on-one, test-on-another (optional)."
   ],
   "metadata": {
    "id": "vabG--6cfRdL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 4_evaluate_cross_domain.ipynb / .py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_SPLIT_DIR = Path(\"work_tamarind\")\n",
    "TEST_CSV = DATA_SPLIT_DIR/\"test.csv\"\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = 384\n",
    "GRID = (3,3)\n",
    "\n",
    "DOMAINS = [\"Shelled\",\"Unshelled\",\"Mixed\"]\n",
    "DOMAIN2ID = {d:i for i,d in enumerate(DOMAINS)}\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.tfm = A.Compose([\n",
    "            A.LongestMaxSize(IMG_SIZE),\n",
    "            A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    def __len__(self): return len(self.df)\n",
    "    def _grid(self, t):\n",
    "        C,H,W = t.shape\n",
    "        gh, gw = GRID\n",
    "        ph, pw = H//gh, W//gw\n",
    "        patches = []\n",
    "        for i in range(gh):\n",
    "            for j in range(gw):\n",
    "                patches.append(t[:, i*ph:(i+1)*ph, j*pw:(j+1)*pw])\n",
    "        return torch.stack(patches, 0)\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        p = r[\"path\"]\n",
    "        y = 0 if r[\"label\"]==\"Healthy\" else 1\n",
    "        d = DOMAIN2ID.get(r[\"domain\"], 0)\n",
    "        with Image.open(p) as im:\n",
    "            im = im.convert(\"RGB\")\n",
    "            arr = np.array(im)\n",
    "        t = self.tfm(image=arr)[\"image\"]\n",
    "        patches = self._grid(t)\n",
    "        return patches, torch.tensor(y).long(), torch.tensor(d).long(), r[\"domain\"], p\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    def __init__(self, backbone=\"mobilenetv3_large_100\", num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0, global_pool=\"avg\")\n",
    "        self.feat_dim = self.backbone.num_features\n",
    "        self.attn = nn.Sequential(nn.Linear(self.feat_dim, 192), nn.ReLU(), nn.Linear(192,1))\n",
    "        self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "    def forward(self, patches):\n",
    "        B,P,C,h,w = patches.shape\n",
    "        x = patches.view(B*P, C, h, w)\n",
    "        f = self.backbone(x).view(B,P,-1)\n",
    "        a = torch.softmax(self.attn(f), dim=1)\n",
    "        bag = (a*f).sum(dim=1)\n",
    "        logits = self.head(bag)\n",
    "        return logits\n",
    "\n",
    "def evaluate(model_path, csv=TEST_CSV):\n",
    "    ds = EvalDataset(csv)\n",
    "    dl = DataLoader(ds, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    model = StudentNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    y_true, y_pred, domains = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for patches, y, d, dom_str, _ in dl:\n",
    "            patches = patches.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            logits = model(patches)\n",
    "            pred = logits.argmax(1)\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(pred.cpu().tolist())\n",
    "            domains.extend(dom_str)\n",
    "\n",
    "    print(\"Overall:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Healthy\",\"Unhealthy\"], digits=4))\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"Acc={acc:.4f}  Macro-F1={mf1:.4f}\")\n",
    "\n",
    "    # Per-domain\n",
    "    per = defaultdict(lambda: {\"y\":[],\"p\":[]})\n",
    "    for yt, yp, dom in zip(y_true, y_pred, domains):\n",
    "        per[dom][\"y\"].append(yt); per[dom][\"p\"].append(yp)\n",
    "    print(\"\\nPer-domain performance:\")\n",
    "    for dom in DOMAINS:\n",
    "        if dom in per:\n",
    "            y = per[dom][\"y\"]; p = per[dom][\"p\"]\n",
    "            acc = accuracy_score(y,p)\n",
    "            mf1 = f1_score(y,p,average=\"macro\")\n",
    "            pr, rc, f1, _ = precision_recall_fscore_support(y,p,average=\"binary\", pos_label=1, zero_division=0)\n",
    "            print(f\"- {dom}: Acc={acc:.4f} Macro-F1={mf1:.4f}  (Unhealthy class) P={pr:.4f} R={rc:.4f} F1={f1:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\"student_mnv3_kd.pt\")\n"
   ],
   "metadata": {
    "id": "n9tz1QRuezDc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "5) Explainability: Grad-CAM + (Optionally) Teacher Attention Maps\n",
    "\n",
    "We apply Grad-CAM to the studentâ€™s final conv stage. If you want teacher attention visualizations, we also render MIL attention weights over the 3Ã—3 patch grid."
   ],
   "metadata": {
    "id": "QKfuVvE8fVLD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 5_explainability.ipynb / .py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 384\n",
    "GRID = (3,3)\n",
    "\n",
    "class StudentForGradCAM(nn.Module):\n",
    "    def __init__(self, backbone=\"mobilenetv3_large_100\", num_classes=2):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0, global_pool=\"avg\")\n",
    "        self.feat_dim = self.backbone.num_features\n",
    "        self.attn = nn.Sequential(nn.Linear(self.feat_dim, 192), nn.ReLU(), nn.Linear(192,1))\n",
    "        self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "        # pick a conv layer for cam\n",
    "        self.target_layer = None\n",
    "        # try to find the last conv\n",
    "        for m in reversed(list(self.backbone.modules())):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                self.target_layer = m\n",
    "                break\n",
    "\n",
    "    def forward(self, patches):\n",
    "        B,P,C,h,w = patches.shape\n",
    "        x = patches.view(B*P, C, h, w)\n",
    "        f = self.backbone(x)                   # [B*P, D]\n",
    "        f = f.view(B,P,-1)\n",
    "        a = torch.softmax(self.attn(f), dim=1) # [B,P,1]\n",
    "        bag = (a*f).sum(dim=1)                 # [B,D]\n",
    "        logits = self.head(bag)\n",
    "        return logits, a\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    tfm = A.Compose([\n",
    "        A.LongestMaxSize(IMG_SIZE),\n",
    "        A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=(0,0,0)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    with Image.open(img_path) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        arr = np.array(im)\n",
    "    out = tfm(image=arr)[\"image\"]  # C,H,W\n",
    "    return arr, out\n",
    "\n",
    "def grid_patches(t):\n",
    "    C,H,W = t.shape\n",
    "    gh,gw = GRID\n",
    "    ph,pw = H//gh, W//gw\n",
    "    patches = []\n",
    "    for i in range(gh):\n",
    "        for j in range(gw):\n",
    "            patches.append(t[:, i*ph:(i+1)*ph, j*pw:(j+1)*pw])\n",
    "    return torch.stack(patches,0)  # [P,C,ph,pw]\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.fmap = None\n",
    "        self.grad = None\n",
    "        target_layer.register_forward_hook(self.fwd_hook)\n",
    "        target_layer.register_full_backward_hook(self.bwd_hook)\n",
    "    def fwd_hook(self, module, inp, out):\n",
    "        self.fmap = out.detach()\n",
    "    def bwd_hook(self, module, grad_in, grad_out):\n",
    "        self.grad = grad_out[0].detach()\n",
    "    def __call__(self, inp, class_idx):\n",
    "        # inp: [B*P, C, h, w] going through backbone only â€” BUT\n",
    "        # our model pools after backbone. We'll do a forward and then backprop.\n",
    "        self.model.zero_grad()\n",
    "        logits, attn = self.model(inp.view(1, *inp.shape))  # hack: treat as [1,P,C,h,w]\n",
    "        score = logits[0, class_idx]\n",
    "        score.backward()\n",
    "        # grad-cam over last conv feature map captured\n",
    "        grads = self.grad       # [?, C, H, W]\n",
    "        fmap  = self.fmap       # same shape\n",
    "        weights = grads.mean(dim=(2,3), keepdim=True)\n",
    "        cam = (weights * fmap).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam[0,0].cpu().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
    "        return cam, attn\n",
    "\n",
    "def overlay_cam_on_image(img_rgb, cam):\n",
    "    cam_resized = cv2.resize(cam, (img_rgb.shape[1], img_rgb.shape[0]))\n",
    "    heatmap = cv2.applyColorMap((cam_resized*255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    overlay = (0.4*heatmap + 0.6*img_rgb[:,:,::-1]).astype(np.uint8)  # BGR mix but final look ok\n",
    "    return overlay[:, :, ::-1]  # back to RGB\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = StudentForGradCAM(num_classes=2).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(\"student_mnv3_kd.pt\", map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    # pick an image to visualize\n",
    "    EXAMPLE_IMG = \"path/to/one/test/image.jpg\"  # <- set one path from test.csv\n",
    "    orig, t = preprocess_image(EXAMPLE_IMG)     # orig: H,W,3  t: C,H,W\n",
    "    patches = grid_patches(t).unsqueeze(0).to(DEVICE)  # [1,P,C,h,w]\n",
    "\n",
    "    cam_engine = GradCAM(model, model.target_layer)\n",
    "    # choose class index: 0 Healthy, 1 Unhealthy\n",
    "    CAM_CLASS = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, attn = model(patches)\n",
    "        pred = logits.argmax(1).item()\n",
    "        print(f\"Pred class: {pred} (0=Healthy,1=Unhealthy)\")\n",
    "\n",
    "    cam, attn_weights = cam_engine(patches.squeeze(0), CAM_CLASS)\n",
    "    vis = overlay_cam_on_image(orig.copy(), cam)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1); plt.imshow(orig); plt.title(\"Original\")\n",
    "    plt.subplot(1,2,2); plt.imshow(vis); plt.title(\"Grad-CAM\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Render MIL attention over 3x3 grid\n",
    "    attn_np = attn.squeeze(-1).squeeze(0).cpu().numpy()  # [P]\n",
    "    attn_np = attn_np.reshape(GRID)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(attn_np, cmap=\"viridis\")\n",
    "    plt.title(\"MIL Attention (3x3 patches)\")\n",
    "    plt.colorbar(); plt.show()\n"
   ],
   "metadata": {
    "id": "cW_OA_-hfV7S"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
